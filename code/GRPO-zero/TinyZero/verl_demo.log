2025-05-13 15:55:54,476	INFO worker.py:1852 -- Started a local Ray instance.
[36m(main_task pid=3180833)[0m {'actor_rollout_ref': {'actor': {'clip_ratio': 0.2,
[36m(main_task pid=3180833)[0m                                  'entropy_coeff': 0.001,
[36m(main_task pid=3180833)[0m                                  'fsdp_config': {'fsdp_size': -1,
[36m(main_task pid=3180833)[0m                                                  'grad_offload': False,
[36m(main_task pid=3180833)[0m                                                  'optimizer_offload': False,
[36m(main_task pid=3180833)[0m                                                  'param_offload': False,
[36m(main_task pid=3180833)[0m                                                  'wrap_policy': {'min_num_params': 0}},
[36m(main_task pid=3180833)[0m                                  'grad_clip': 1.0,
[36m(main_task pid=3180833)[0m                                  'kl_loss_coef': 0.001,
[36m(main_task pid=3180833)[0m                                  'kl_loss_type': 'low_var_kl',
[36m(main_task pid=3180833)[0m                                  'optim': {'lr': 1e-06,
[36m(main_task pid=3180833)[0m                                            'lr_warmup_steps_ratio': 0.0,
[36m(main_task pid=3180833)[0m                                            'min_lr_ratio': None,
[36m(main_task pid=3180833)[0m                                            'total_training_steps': -1,
[36m(main_task pid=3180833)[0m                                            'warmup_style': 'constant'},
[36m(main_task pid=3180833)[0m                                  'ppo_epochs': 1,
[36m(main_task pid=3180833)[0m                                  'ppo_max_token_len_per_gpu': 16384,
[36m(main_task pid=3180833)[0m                                  'ppo_micro_batch_size': 8,
[36m(main_task pid=3180833)[0m                                  'ppo_mini_batch_size': 32,
[36m(main_task pid=3180833)[0m                                  'shuffle': False,
[36m(main_task pid=3180833)[0m                                  'strategy': 'fsdp',
[36m(main_task pid=3180833)[0m                                  'ulysses_sequence_parallel_size': 1,
[36m(main_task pid=3180833)[0m                                  'use_dynamic_bsz': False,
[36m(main_task pid=3180833)[0m                                  'use_kl_loss': True},
[36m(main_task pid=3180833)[0m                        'hybrid_engine': True,
[36m(main_task pid=3180833)[0m                        'model': {'enable_gradient_checkpointing': True,
[36m(main_task pid=3180833)[0m                                  'external_lib': None,
[36m(main_task pid=3180833)[0m                                  'override_config': {},
[36m(main_task pid=3180833)[0m                                  'path': '/home/wuyicong/wyc/graph_reasoning/model/DeepSeek_R1_Distill_Qwen_1.5B',
[36m(main_task pid=3180833)[0m                                  'use_remove_padding': True},
[36m(main_task pid=3180833)[0m                        'ref': {'fsdp_config': {'fsdp_size': -1,
[36m(main_task pid=3180833)[0m                                                'param_offload': True,
[36m(main_task pid=3180833)[0m                                                'wrap_policy': {'min_num_params': 0}},
[36m(main_task pid=3180833)[0m                                'log_prob_max_token_len_per_gpu': 16384,
[36m(main_task pid=3180833)[0m                                'log_prob_micro_batch_size': 8,
[36m(main_task pid=3180833)[0m                                'log_prob_use_dynamic_bsz': False,
[36m(main_task pid=3180833)[0m                                'ulysses_sequence_parallel_size': 1},
[36m(main_task pid=3180833)[0m                        'rollout': {'do_sample': True,
[36m(main_task pid=3180833)[0m                                    'dtype': 'bfloat16',
[36m(main_task pid=3180833)[0m                                    'enforce_eager': True,
[36m(main_task pid=3180833)[0m                                    'free_cache_engine': True,
[36m(main_task pid=3180833)[0m                                    'gpu_memory_utilization': 0.6,
[36m(main_task pid=3180833)[0m                                    'ignore_eos': False,
[36m(main_task pid=3180833)[0m                                    'load_format': 'dummy_dtensor',
[36m(main_task pid=3180833)[0m                                    'log_prob_max_token_len_per_gpu': 16384,
[36m(main_task pid=3180833)[0m                                    'log_prob_micro_batch_size': 8,
[36m(main_task pid=3180833)[0m                                    'log_prob_use_dynamic_bsz': False,
[36m(main_task pid=3180833)[0m                                    'max_num_batched_tokens': 8192,
[36m(main_task pid=3180833)[0m                                    'max_num_seqs': 1024,
[36m(main_task pid=3180833)[0m                                    'n': 5,
[36m(main_task pid=3180833)[0m                                    'name': 'vllm',
[36m(main_task pid=3180833)[0m                                    'prompt_length': 2048,
[36m(main_task pid=3180833)[0m                                    'response_length': 4096,
[36m(main_task pid=3180833)[0m                                    'temperature': 1.0,
[36m(main_task pid=3180833)[0m                                    'tensor_model_parallel_size': 2,
[36m(main_task pid=3180833)[0m                                    'top_k': -1,
[36m(main_task pid=3180833)[0m                                    'top_p': 1}},
[36m(main_task pid=3180833)[0m  'algorithm': {'adv_estimator': 'grpo',
[36m(main_task pid=3180833)[0m                'gamma': 1.0,
[36m(main_task pid=3180833)[0m                'kl_ctrl': {'kl_coef': 0.001, 'type': 'fixed'},
[36m(main_task pid=3180833)[0m                'kl_penalty': 'kl',
[36m(main_task pid=3180833)[0m                'lam': 1.0},
[36m(main_task pid=3180833)[0m  'critic': {'cliprange_value': 0.5,
[36m(main_task pid=3180833)[0m             'forward_max_token_len_per_gpu': 32768,
[36m(main_task pid=3180833)[0m             'forward_micro_batch_size': 64,
[36m(main_task pid=3180833)[0m             'grad_clip': 1.0,
[36m(main_task pid=3180833)[0m             'model': {'enable_gradient_checkpointing': False,
[36m(main_task pid=3180833)[0m                       'external_lib': None,
[36m(main_task pid=3180833)[0m                       'fsdp_config': {'fsdp_size': -1,
[36m(main_task pid=3180833)[0m                                       'grad_offload': False,
[36m(main_task pid=3180833)[0m                                       'optimizer_offload': False,
[36m(main_task pid=3180833)[0m                                       'param_offload': False,
[36m(main_task pid=3180833)[0m                                       'wrap_policy': {'min_num_params': 0}},
[36m(main_task pid=3180833)[0m                       'override_config': {},
[36m(main_task pid=3180833)[0m                       'path': '~/models/deepseek-llm-7b-chat',
[36m(main_task pid=3180833)[0m                       'tokenizer_path': '/home/wuyicong/wyc/graph_reasoning/model/DeepSeek_R1_Distill_Qwen_1.5B',
[36m(main_task pid=3180833)[0m                       'use_remove_padding': False},
[36m(main_task pid=3180833)[0m             'optim': {'lr': 1e-05,
[36m(main_task pid=3180833)[0m                       'lr_warmup_steps_ratio': 0.0,
[36m(main_task pid=3180833)[0m                       'min_lr_ratio': None,
[36m(main_task pid=3180833)[0m                       'total_training_steps': -1,
[36m(main_task pid=3180833)[0m                       'warmup_style': 'constant'},
[36m(main_task pid=3180833)[0m             'ppo_epochs': 1,
[36m(main_task pid=3180833)[0m             'ppo_max_token_len_per_gpu': 32768,
[36m(main_task pid=3180833)[0m             'ppo_micro_batch_size': 64,
[36m(main_task pid=3180833)[0m             'ppo_mini_batch_size': 32,
[36m(main_task pid=3180833)[0m             'shuffle': False,
[36m(main_task pid=3180833)[0m             'strategy': 'fsdp',
[36m(main_task pid=3180833)[0m             'ulysses_sequence_parallel_size': 1,
[36m(main_task pid=3180833)[0m             'use_dynamic_bsz': False},
[36m(WorkerDict pid=3181251)[0m Loading checkpoint shards:   0%|          | 0/2 [00:00<?, ?it/s]
[36m(WorkerDict pid=3181490)[0m Loading checkpoint shards:  50%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆ     | 1/2 [00:01<00:01,  1.55s/it]
[36m(WorkerDict pid=3181490)[0m Loading checkpoint shards: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 2/2 [00:02<00:00,  1.09it/s]Loading checkpoint shards: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 2/2 [00:02<00:00,  1.01s/it]
[36m(WorkerDict pid=3181490)[0m Loading checkpoint shards:   0%|          | 0/2 [00:00<?, ?it/s]
[36m(WorkerDict pid=3181490)[0m Loading checkpoint shards:   0%|          | 0/2 [00:00<?, ?it/s]
[36m(WorkerDict pid=3181251)[0m Loading checkpoint shards:  50%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆ     | 1/2 [00:04<00:04,  4.76s/it]
[36m(WorkerDict pid=3181251)[0m Loading checkpoint shards: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 2/2 [00:05<00:00,  2.49s/it]Loading checkpoint shards: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 2/2 [00:05<00:00,  2.83s/it]
[36m(WorkerDict pid=3181490)[0m Loading checkpoint shards:  50%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆ     | 1/2 [00:00<00:00,  4.50it/s]
[36m(WorkerDict pid=3181490)[0m Loading checkpoint shards: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 2/2 [00:00<00:00,  5.06it/s]Loading checkpoint shards: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 2/2 [00:00<00:00,  4.96it/s]
[36m(main_task pid=3180833)[0m  'data': {'max_prompt_length': 2048,
[36m(main_task pid=3180833)[0m           'max_response_length': 4096,
[36m(main_task pid=3180833)[0m           'prompt_key': 'prompt',
[36m(main_task pid=3180833)[0m           'return_raw_chat': False,
[36m(main_task pid=3180833)[0m           'return_raw_input_ids': False,
[36m(main_task pid=3180833)[0m           'tokenizer': None,
[36m(main_task pid=3180833)[0m           'train_batch_size': 64,
[36m(main_task pid=3180833)[0m           'train_files': './graph/data/train_8422_rethink.parquet',
[36m(main_task pid=3180833)[0m           'val_batch_size': 32,
[36m(main_task pid=3180833)[0m           'val_files': './graph/data/test_900_rethink.parquet'},
[36m(main_task pid=3180833)[0m  'reward_model': {'enable': False,
[36m(main_task pid=3180833)[0m                   'forward_max_token_len_per_gpu': 32768,
[36m(main_task pid=3180833)[0m                   'max_length': None,
[36m(main_task pid=3180833)[0m                   'micro_batch_size': 64,
[36m(main_task pid=3180833)[0m                   'model': {'external_lib': None,
[36m(main_task pid=3180833)[0m                             'fsdp_config': {'min_num_params': 0,
[36m(main_task pid=3180833)[0m                                             'param_offload': False},
[36m(main_task pid=3180833)[0m                             'input_tokenizer': '/home/wuyicong/wyc/graph_reasoning/model/DeepSeek_R1_Distill_Qwen_1.5B',
[36m(main_task pid=3180833)[0m                             'path': '~/models/FsfairX-LLaMA3-RM-v0.1',
[36m(main_task pid=3180833)[0m                             'use_remove_padding': False},
[36m(main_task pid=3180833)[0m                   'strategy': 'fsdp',
[36m(main_task pid=3180833)[0m                   'ulysses_sequence_parallel_size': 1,
[36m(main_task pid=3180833)[0m                   'use_dynamic_bsz': False},
[36m(main_task pid=3180833)[0m  'trainer': {'critic_warmup': 0,
[36m(main_task pid=3180833)[0m              'default_hdfs_dir': None,
[36m(main_task pid=3180833)[0m              'default_local_dir': './outputs/grpo-rethink-new-1.5B',
[36m(main_task pid=3180833)[0m              'experiment_name': 'grpo-rethink-new-1.5B',
[36m(main_task pid=3180833)[0m              'logger': ['wandb'],
[36m(main_task pid=3180833)[0m              'n_gpus_per_node': 2,
[36m(main_task pid=3180833)[0m              'nnodes': 1,
[36m(main_task pid=3180833)[0m              'project_name': 'TinyZero',
[36m(main_task pid=3180833)[0m              'save_best_freq': 10,
[36m(main_task pid=3180833)[0m              'save_freq': 3,
[36m(main_task pid=3180833)[0m              'save_top_k': 3,
[36m(main_task pid=3180833)[0m              'save_warm_up': 0,
[36m(main_task pid=3180833)[0m              'test_freq': 3,
[36m(main_task pid=3180833)[0m              'total_epochs': 2,
[36m(main_task pid=3180833)[0m              'total_training_steps': None,
[36m(main_task pid=3180833)[0m              'val_before_train': True}}
[36m(main_task pid=3180833)[0m original dataset len: 8422
[36m(main_task pid=3180833)[0m filter dataset len: 8422
[36m(main_task pid=3180833)[0m original dataset len: 900
[36m(main_task pid=3180833)[0m filter dataset len: 900
[36m(main_task pid=3180833)[0m Size of train dataloader: 131
[36m(main_task pid=3180833)[0m Size of val dataloader: 1
[36m(main_task pid=3180833)[0m Total training steps: 262
[36m(WorkerDict pid=3181251)[0m Model config after override: Qwen2Config {
[36m(WorkerDict pid=3181251)[0m   "_name_or_path": "/home/wuyicong/wyc/graph_reasoning/model/DeepSeek_R1_Distill_Qwen_1.5B",
[36m(WorkerDict pid=3181251)[0m   "architectures": [
[36m(WorkerDict pid=3181251)[0m     "Qwen2ForCausalLM"
[36m(WorkerDict pid=3181251)[0m   ],
[36m(WorkerDict pid=3181251)[0m   "attention_dropout": 0.0,
[36m(WorkerDict pid=3181251)[0m   "bos_token_id": 151646,
[36m(WorkerDict pid=3181251)[0m   "eos_token_id": 151643,
[36m(WorkerDict pid=3181251)[0m   "hidden_act": "silu",
[36m(WorkerDict pid=3181251)[0m   "hidden_size": 1536,
[36m(WorkerDict pid=3181251)[0m   "initializer_range": 0.02,
[36m(WorkerDict pid=3181251)[0m   "intermediate_size": 8960,
[36m(WorkerDict pid=3181251)[0m   "max_position_embeddings": 131072,
[36m(WorkerDict pid=3181251)[0m   "max_window_layers": 21,
[36m(WorkerDict pid=3181251)[0m   "model_type": "qwen2",
[36m(WorkerDict pid=3181251)[0m   "num_attention_heads": 12,
[36m(WorkerDict pid=3181251)[0m   "num_hidden_layers": 28,
[36m(WorkerDict pid=3181251)[0m   "num_key_value_heads": 2,
[36m(WorkerDict pid=3181251)[0m   "pad_token_id": 151643,
[36m(WorkerDict pid=3181251)[0m   "rms_norm_eps": 1e-06,
[36m(WorkerDict pid=3181251)[0m   "rope_scaling": null,
[36m(WorkerDict pid=3181251)[0m   "rope_theta": 10000,
[36m(WorkerDict pid=3181251)[0m   "sliding_window": null,
[36m(WorkerDict pid=3181251)[0m   "tie_word_embeddings": false,
[36m(WorkerDict pid=3181251)[0m   "torch_dtype": "float32",
[36m(WorkerDict pid=3181251)[0m   "transformers_version": "4.47.1",
[36m(WorkerDict pid=3181251)[0m   "use_cache": true,
[36m(WorkerDict pid=3181251)[0m   "use_mrope": false,
[36m(WorkerDict pid=3181251)[0m   "use_sliding_window": false,
[36m(WorkerDict pid=3181251)[0m   "vocab_size": 151936
[36m(WorkerDict pid=3181251)[0m }
[36m(WorkerDict pid=3181251)[0m 
[36m(WorkerDict pid=3181251)[0m NCCL version 2.20.5+cuda12.4
[36m(WorkerDict pid=3181251)[0m Qwen2ForCausalLM contains 1.78B parameters
[36m(WorkerDict pid=3181251)[0m wrap_policy: functools.partial(<function transformer_auto_wrap_policy at 0x7f883046d3a0>, transformer_layer_cls={<class 'transformers.models.qwen2.modeling_qwen2.Qwen2DecoderLayer'>})
[36m(WorkerDict pid=3181490)[0m Actor use_remove_padding=True
[36m(WorkerDict pid=3181490)[0m wrap_policy: functools.partial(<function transformer_auto_wrap_policy at 0x7f90dcc0d3a0>, transformer_layer_cls={<class 'transformers.models.qwen2.modeling_qwen2.Qwen2DecoderLayer'>})
[36m(WorkerDict pid=3181251)[0m Model config after override: Qwen2Config {
[36m(WorkerDict pid=3181251)[0m   "_name_or_path": "/home/wuyicong/wyc/graph_reasoning/model/DeepSeek_R1_Distill_Qwen_1.5B",
[36m(WorkerDict pid=3181251)[0m   "architectures": [
[36m(WorkerDict pid=3181251)[0m     "Qwen2ForCausalLM"
[36m(WorkerDict pid=3181251)[0m   ],
[36m(WorkerDict pid=3181251)[0m   "attention_dropout": 0.0,
[36m(WorkerDict pid=3181251)[0m   "bos_token_id": 151646,
[36m(WorkerDict pid=3181251)[0m   "eos_token_id": 151643,
[36m(WorkerDict pid=3181251)[0m   "hidden_act": "silu",
[36m(WorkerDict pid=3181251)[0m   "hidden_size": 1536,
[36m(WorkerDict pid=3181251)[0m   "initializer_range": 0.02,
[36m(WorkerDict pid=3181251)[0m   "intermediate_size": 8960,
[36m(WorkerDict pid=3181251)[0m   "max_position_embeddings": 131072,
[36m(WorkerDict pid=3181251)[0m   "max_window_layers": 21,
[36m(WorkerDict pid=3181251)[0m   "model_type": "qwen2",
[36m(WorkerDict pid=3181251)[0m   "num_attention_heads": 12,
[36m(WorkerDict pid=3181251)[0m   "num_hidden_layers": 28,
[36m(WorkerDict pid=3181251)[0m   "num_key_value_heads": 2,
[36m(WorkerDict pid=3181251)[0m   "pad_token_id": 151643,
[36m(WorkerDict pid=3181251)[0m   "rms_norm_eps": 1e-06,
[36m(WorkerDict pid=3181251)[0m   "rope_scaling": null,
[36m(WorkerDict pid=3181251)[0m   "rope_theta": 10000,
[36m(WorkerDict pid=3181251)[0m   "sliding_window": null,
[36m(WorkerDict pid=3181251)[0m   "tie_word_embeddings": false,
[36m(WorkerDict pid=3181251)[0m   "torch_dtype": "float32",
[36m(WorkerDict pid=3181251)[0m   "transformers_version": "4.47.1",
[36m(WorkerDict pid=3181251)[0m   "use_cache": true,
[36m(WorkerDict pid=3181251)[0m   "use_mrope": false,
[36m(WorkerDict pid=3181251)[0m   "use_sliding_window": false,
[36m(WorkerDict pid=3181251)[0m   "vocab_size": 151936
[36m(WorkerDict pid=3181251)[0m }
[36m(WorkerDict pid=3181251)[0m 
[36m(WorkerDict pid=3181251)[0m Actor use_remove_padding=True
[36m(WorkerDict pid=3181251)[0m Qwen2ForCausalLM contains 1.78B parameters
[36m(WorkerDict pid=3181490)[0m /home/wuyicong/conda/envs/zero/lib/python3.9/site-packages/xformers/ops/fmha/flash.py:211: FutureWarning: `torch.library.impl_abstract` was renamed to `torch.library.register_fake`. Please use that instead; we will remove `torch.library.impl_abstract` in a future version of PyTorch.
[36m(WorkerDict pid=3181490)[0m   @torch.library.impl_abstract("xformers_flash::flash_fwd")
[36m(WorkerDict pid=3181251)[0m Loading checkpoint shards:   0%|          | 0/2 [00:00<?, ?it/s]
[36m(WorkerDict pid=3181251)[0m Loading checkpoint shards:  50%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆ     | 1/2 [00:00<00:00,  4.10it/s]
[36m(WorkerDict pid=3181251)[0m Loading checkpoint shards: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 2/2 [00:00<00:00,  4.61it/s]Loading checkpoint shards: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 2/2 [00:00<00:00,  4.52it/s]
[36m(WorkerDict pid=3181490)[0m /home/wuyicong/conda/envs/zero/lib/python3.9/site-packages/xformers/ops/fmha/flash.py:344: FutureWarning: `torch.library.impl_abstract` was renamed to `torch.library.register_fake`. Please use that instead; we will remove `torch.library.impl_abstract` in a future version of PyTorch.
[36m(WorkerDict pid=3181490)[0m   @torch.library.impl_abstract("xformers_flash::flash_bwd")
[36m(WorkerDict pid=3181251)[0m /home/wuyicong/conda/envs/zero/lib/python3.9/site-packages/torch/distributed/fsdp/fully_sharded_data_parallel.py:689: FutureWarning: FSDP.state_dict_type() and FSDP.set_state_dict_type() are being deprecated. Please use APIs, get_state_dict() and set_state_dict(), which can support different parallelisms, FSDP1, FSDP2, DDP. API doc: https://pytorch.org/docs/stable/distributed.checkpoint.html#torch.distributed.checkpoint.state_dict.get_state_dict .Tutorial: https://pytorch.org/tutorials/recipes/distributed_checkpoint_recipe.html .
[36m(WorkerDict pid=3181251)[0m   warnings.warn(
[36m(WorkerDict pid=3181251)[0m /home/wuyicong/conda/envs/zero/lib/python3.9/site-packages/xformers/ops/fmha/flash.py:344: FutureWarning: `torch.library.impl_abstract` was renamed to `torch.library.register_fake`. Please use that instead; we will remove `torch.library.impl_abstract` in a future version of PyTorch.[32m [repeated 2x across cluster] (Ray deduplicates logs by default. Set RAY_DEDUP_LOGS=0 to disable log deduplication, or see https://docs.ray.io/en/master/ray-observability/user-guides/configure-logging.html#log-deduplication for more options.)[0m
[36m(WorkerDict pid=3181251)[0m   @torch.library.impl_abstract("xformers_flash::flash_fwd")
[36m(WorkerDict pid=3181251)[0m   @torch.library.impl_abstract("xformers_flash::flash_bwd")
[36m(main_task pid=3180833)[0m wandb: Using wandb-core as the SDK backend.  Please refer to https://wandb.me/wandb-core for more information.
[36m(main_task pid=3180833)[0m wandb: Appending key for api.wandb.ai to your netrc file: /root/.netrc
[36m(main_task pid=3180833)[0m wandb: Currently logged in as: 1625387092 (1625387092-university) to https://api.wandb.ai. Use `wandb login --relogin` to force relogin
[36m(main_task pid=3180833)[0m wandb: creating run
[36m(WorkerDict pid=3181490)[0m /home/wuyicong/conda/envs/zero/lib/python3.9/site-packages/torch/distributed/fsdp/fully_sharded_data_parallel.py:689: FutureWarning: FSDP.state_dict_type() and FSDP.set_state_dict_type() are being deprecated. Please use APIs, get_state_dict() and set_state_dict(), which can support different parallelisms, FSDP1, FSDP2, DDP. API doc: https://pytorch.org/docs/stable/distributed.checkpoint.html#torch.distributed.checkpoint.state_dict.get_state_dict .Tutorial: https://pytorch.org/tutorials/recipes/distributed_checkpoint_recipe.html .
[36m(WorkerDict pid=3181490)[0m   warnings.warn(
[36m(main_task pid=3180833)[0m wandb: Tracking run with wandb version 0.19.9
[36m(main_task pid=3180833)[0m wandb: Run data is saved locally in /home/wuyicong/wyc/graph_reasoning/GRPO-zero/TinyZero/wandb/run-20250513_155831-btpoq9zg
[36m(main_task pid=3180833)[0m wandb: Run `wandb offline` to turn off syncing.
[36m(main_task pid=3180833)[0m wandb: Syncing run grpo-rethink-new-1.5B
[36m(main_task pid=3180833)[0m wandb: â­ï¸ View project at https://wandb.ai/1625387092-university/TinyZero
[36m(main_task pid=3180833)[0m wandb: ðŸš€ View run at https://wandb.ai/1625387092-university/TinyZero/runs/btpoq9zg
[36m(WorkerDict pid=3181251)[0m wrap_policy: functools.partial(<function transformer_auto_wrap_policy at 0x7f883046d3a0>, transformer_layer_cls={<class 'transformers.models.qwen2.modeling_qwen2.Qwen2DecoderLayer'>})
[36m(WorkerDict pid=3181490)[0m Total steps: 262, num_warmup_steps: 0
[36m(WorkerDict pid=3181490)[0m Actor use_remove_padding=True
[36m(WorkerDict pid=3181490)[0m INFO 05-13 15:57:49 config.py:1450] Downcasting torch.float32 to torch.bfloat16.
[36m(WorkerDict pid=3181490)[0m INFO 05-13 15:57:49 config.py:729] Defaulting to use ray for distributed inference
[36m(WorkerDict pid=3181490)[0m wrap_policy: functools.partial(<function transformer_auto_wrap_policy at 0x7f90dcc0d3a0>, transformer_layer_cls={<class 'transformers.models.qwen2.modeling_qwen2.Qwen2DecoderLayer'>})
[36m(WorkerDict pid=3181490)[0m local rank 0
[36m(WorkerDict pid=3181490)[0m INFO 05-13 15:57:49 selector.py:54] Using XFormers backend.
[36m(WorkerDict pid=3181251)[0m Before building vllm rollout, memory allocated (GB): 3.330353260040283, memory reserved (GB): 9.107421875
[36m(WorkerDict pid=3181251)[0m INFO 05-13 15:57:52 utils.py:841] Found nccl from library libnccl.so.2
[36m(WorkerDict pid=3181251)[0m INFO 05-13 15:57:52 pynccl.py:63] vLLM is using nccl==2.20.5
[36m(WorkerDict pid=3181251)[0m INFO 05-13 15:57:55 shm_broadcast.py:235] vLLM message queue communication handle: Handle(connect_ip='127.0.0.1', local_reader_ranks=[1], buffer=<vllm.distributed.device_communicators.shm_broadcast.ShmRingBuffer object at 0x7f876cf1aee0>, local_subscribe_port=44717, remote_subscribe_port=None)
[36m(WorkerDict pid=3181251)[0m Total steps: 262, num_warmup_steps: 0
[36m(WorkerDict pid=3181251)[0m Actor use_remove_padding=True
[36m(WorkerDict pid=3181251)[0m INFO 05-13 15:57:50 config.py:1450] Downcasting torch.float32 to torch.bfloat16.
[36m(WorkerDict pid=3181251)[0m INFO 05-13 15:57:50 config.py:729] Defaulting to use ray for distributed inference
[36m(WorkerDict pid=3181251)[0m local rank 0
[36m(WorkerDict pid=3181251)[0m INFO 05-13 15:57:50 selector.py:54] Using XFormers backend.
[36m(WorkerDict pid=3181251)[0m INFO 05-13 15:57:56 selector.py:54] Using XFormers backend.
[36m(WorkerDict pid=3181251)[0m before init cache memory allocated: 5.399124992GB, reserved: 5.505024GB
[36m(WorkerDict pid=3181490)[0m INFO 05-13 15:57:52 utils.py:841] Found nccl from library libnccl.so.2
[36m(WorkerDict pid=3181490)[0m INFO 05-13 15:57:52 pynccl.py:63] vLLM is using nccl==2.20.5
[36m(WorkerDict pid=3181490)[0m INFO 05-13 15:57:56 selector.py:54] Using XFormers backend.
[36m(WorkerDict pid=3181251)[0m after init cache memory allocated: 10.683948032GB, reserved: 10.78984704GB
[36m(WorkerDict pid=3181251)[0m kwargs: {'n': 5, 'logprobs': 1, 'max_tokens': 4096, 'detokenize': False, 'temperature': 1.0, 'top_k': -1, 'top_p': 1, 'ignore_eos': False}
[36m(WorkerDict pid=3181251)[0m After building vllm rollout, memory allocated (GB): 8.29141616821289, memory reserved (GB): 10.048828125
[36m(WorkerDict pid=3181251)[0m After building sharding manager, memory allocated (GB): 8.29141616821289, memory reserved (GB): 10.048828125
Error executing job with overrides: ['algorithm.adv_estimator=grpo', 'data.train_files=./graph/data/train_8422_rethink.parquet', 'data.val_files=./graph/data/test_900_rethink.parquet', 'data.train_batch_size=64', 'data.val_batch_size=32', 'data.max_prompt_length=2048', 'data.max_response_length=4096', 'actor_rollout_ref.model.path=/home/wuyicong/wyc/graph_reasoning/model/DeepSeek_R1_Distill_Qwen_1.5B', 'actor_rollout_ref.actor.optim.lr=1e-6', 'actor_rollout_ref.model.use_remove_padding=True', 'actor_rollout_ref.actor.ppo_mini_batch_size=32', 'actor_rollout_ref.actor.ppo_micro_batch_size=8', 'actor_rollout_ref.actor.use_kl_loss=True', 'actor_rollout_ref.actor.kl_loss_coef=0.001', 'actor_rollout_ref.actor.kl_loss_type=low_var_kl', 'actor_rollout_ref.model.enable_gradient_checkpointing=True', 'actor_rollout_ref.actor.fsdp_config.param_offload=False', 'actor_rollout_ref.actor.fsdp_config.grad_offload=False', 'actor_rollout_ref.actor.fsdp_config.optimizer_offload=False', 'actor_rollout_ref.rollout.log_prob_micro_batch_size=8', 'actor_rollout_ref.rollout.tensor_model_parallel_size=2', 'actor_rollout_ref.rollout.name=vllm', 'actor_rollout_ref.rollout.gpu_memory_utilization=0.6', 'actor_rollout_ref.rollout.n=5', 'actor_rollout_ref.ref.log_prob_micro_batch_size=8', 'actor_rollout_ref.ref.fsdp_config.param_offload=True', 'algorithm.kl_ctrl.kl_coef=0.001', 'trainer.critic_warmup=0', 'trainer.logger=[wandb]', '+trainer.val_before_train=False', 'trainer.default_hdfs_dir=null', 'trainer.n_gpus_per_node=2', 'trainer.nnodes=1', 'trainer.save_freq=3', 'trainer.test_freq=3', 'trainer.project_name=TinyZero', 'trainer.experiment_name=grpo-rethink-new-1.5B', 'trainer.default_local_dir=./outputs/grpo-rethink-new-1.5B', '+trainer.save_top_k=3', '+trainer.save_best_freq=10', '+trainer.save_warm_up=0', 'trainer.val_before_train=True', 'trainer.total_epochs=2']
Traceback (most recent call last):
  File "/home/wuyicong/wyc/graph_reasoning/GRPO-zero/TinyZero/verl/trainer/main_ppo.py", line 233, in main
    ray.get(main_task.remote(config))
  File "/home/wuyicong/conda/envs/zero/lib/python3.9/site-packages/ray/_private/auto_init_hook.py", line 21, in auto_init_wrapper
    return fn(*args, **kwargs)
  File "/home/wuyicong/conda/envs/zero/lib/python3.9/site-packages/ray/_private/client_mode_hook.py", line 103, in wrapper
    return func(*args, **kwargs)
  File "/home/wuyicong/conda/envs/zero/lib/python3.9/site-packages/ray/_private/worker.py", line 2782, in get
    values, debugger_breakpoint = worker.get_objects(object_refs, timeout=timeout)
  File "/home/wuyicong/conda/envs/zero/lib/python3.9/site-packages/ray/_private/worker.py", line 929, in get_objects
    raise value.as_instanceof_cause()
ray.exceptions.RayTaskError(OutOfMemoryError): [36mray::main_task()[39m (pid=3180833, ip=10.134.10.105)
  File "/home/wuyicong/wyc/graph_reasoning/GRPO-zero/TinyZero/verl/trainer/main_ppo.py", line 319, in main_task
    trainer.fit()
  File "/home/wuyicong/wyc/graph_reasoning/GRPO-zero/TinyZero/verl/trainer/ppo/ray_trainer.py", line 569, in fit
    val_metrics = self._validate()
  File "/home/wuyicong/wyc/graph_reasoning/GRPO-zero/TinyZero/verl/trainer/ppo/ray_trainer.py", line 417, in _validate
    test_output_gen_batch_padded = self.actor_rollout_wg.generate_sequences(test_gen_batch_padded)
  File "/home/wuyicong/wyc/graph_reasoning/GRPO-zero/TinyZero/verl/single_controller/ray/base.py", line 42, in func
    output = ray.get(output)
ray.exceptions.RayTaskError(OutOfMemoryError): [36mray::WorkerDict.actor_rollout_generate_sequences()[39m (pid=3181251, ip=10.134.10.105, actor_id=f02e768ca5f5d12e0f29763101000000, repr=<verl.single_controller.ray.base.WorkerDict object at 0x7f87cf1c3220>)
  File "/home/wuyicong/wyc/graph_reasoning/GRPO-zero/TinyZero/verl/single_controller/ray/base.py", line 399, in func
    return getattr(self.worker_dict[key], name)(*args, **kwargs)
  File "/home/wuyicong/wyc/graph_reasoning/GRPO-zero/TinyZero/verl/single_controller/base/decorator.py", line 404, in inner
    return func(*args, **kwargs)
  File "/home/wuyicong/wyc/graph_reasoning/GRPO-zero/TinyZero/verl/workers/fsdp_workers.py", line 415, in generate_sequences
    with self.rollout_sharding_manager:
  File "/home/wuyicong/wyc/graph_reasoning/GRPO-zero/TinyZero/verl/workers/sharding_manager/fsdp_vllm.py", line 71, in __enter__
    params = self.module.state_dict()
  File "/home/wuyicong/conda/envs/zero/lib/python3.9/site-packages/torch/nn/modules/module.py", line 1941, in state_dict
    hook_result = hook(self, destination, prefix, local_metadata)
  File "/home/wuyicong/conda/envs/zero/lib/python3.9/site-packages/torch/utils/_contextlib.py", line 116, in decorate_context
    return func(*args, **kwargs)
  File "/home/wuyicong/conda/envs/zero/lib/python3.9/site-packages/torch/distributed/fsdp/_state_dict_utils.py", line 729, in _post_state_dict_hook
    processed_state_dict = _post_state_dict_hook_fn[fsdp_state._state_dict_type](
  File "/home/wuyicong/conda/envs/zero/lib/python3.9/site-packages/torch/distributed/fsdp/_state_dict_utils.py", line 574, in _sharded_post_state_dict_hook
    return _common_unshard_post_state_dict_hook(
  File "/home/wuyicong/conda/envs/zero/lib/python3.9/site-packages/torch/distributed/fsdp/_state_dict_utils.py", line 243, in _common_unshard_post_state_dict_hook
    param_hook(state_dict, prefix, fqn)
  File "/home/wuyicong/conda/envs/zero/lib/python3.9/site-packages/torch/distributed/fsdp/_state_dict_utils.py", line 564, in param_hook
    sharded_tensor = _ext_chunk_dtensor(
  File "/home/wuyicong/conda/envs/zero/lib/python3.9/site-packages/torch/distributed/fsdp/_fsdp_extensions.py", line 150, in _ext_chunk_dtensor
    return chunk_dtensor_fn(
  File "/home/wuyicong/conda/envs/zero/lib/python3.9/site-packages/torch/distributed/fsdp/_shard_utils.py", line 107, in _create_chunk_dtensor
    return DTensor.from_local(
  File "/home/wuyicong/conda/envs/zero/lib/python3.9/site-packages/torch/distributed/_tensor/api.py", line 483, in redistribute
    return Redistribute.apply(self, device_mesh, placements, async_op)
  File "/home/wuyicong/conda/envs/zero/lib/python3.9/site-packages/torch/autograd/function.py", line 574, in apply
    return super().apply(*args, **kwargs)  # type: ignore[misc]
  File "/home/wuyicong/conda/envs/zero/lib/python3.9/site-packages/torch/distributed/_tensor/_redistribute.py", line 282, in forward
    output = redistribute_local_tensor(
  File "/home/wuyicong/conda/envs/zero/lib/python3.9/site-packages/torch/distributed/_tensor/_redistribute.py", line 206, in redistribute_local_tensor
    new_local_tensor = target_placement._replicate_to_shard(
  File "/home/wuyicong/conda/envs/zero/lib/python3.9/site-packages/torch/distributed/_tensor/placement_types.py", line 262, in _replicate_to_shard
    return shards[shard_index].clone()
torch.OutOfMemoryError: CUDA out of memory. Tried to allocate 446.00 MiB. GPU 0 has a total capacity of 79.14 GiB of which 435.12 MiB is free. Process 2624995 has 22.87 GiB memory in use. Process 2918252 has 40.99 GiB memory in use. Process 2953189 has 14.79 GiB memory in use. Of the allocated memory 13.80 GiB is allocated by PyTorch, and 327.39 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)

Set the environment variable HYDRA_FULL_ERROR=1 for a complete stack trace.
[36m(WorkerDict pid=3181490)[0m kwargs: {'n': 5, 'logprobs': 1, 'max_tokens': 4096, 'detokenize': False, 'temperature': 1.0, 'top_k': -1, 'top_p': 1, 'ignore_eos': False}
